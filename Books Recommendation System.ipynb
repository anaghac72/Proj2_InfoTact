{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82801d68-d3a5-49ed-a2fe-75947c4f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350695a9-c6cf-4d66-b9a4-9a00e57923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main files\n",
    "books = pd.read_csv(\"books.csv\")\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "book_tags = pd.read_csv(\"book_tags.csv\")\n",
    "tags = pd.read_csv(\"tags.csv\")\n",
    "to_read = pd.read_csv(\"to_read.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1fa84-4f3d-4982-8a63-2732cab0028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows and basic info\n",
    "print(\"Books Data:\")\n",
    "print(books.info(),\"\\n\")\n",
    "\n",
    "print(\"Ratings Data:\")\n",
    "print(ratings.info(),\"\\n\")\n",
    "\n",
    "print(\"Book Tags Data:\")\n",
    "print(book_tags.info(), \"\\n\")\n",
    "\n",
    "print(\"Tags Data:\")\n",
    "print(tags.info(), \"\\n\")\n",
    "\n",
    "print(\"To Read Data:\")\n",
    "print(to_read.info(),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118e883-e824-41b9-8127-f9cfa3926110",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4871b-1e52-4ccf-ba98-a13b02f33e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317fe1a-49d8-4370-ad27-7adbdf9cda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9c8ce-f312-4afb-9755-91c11c153583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62dc2a-26f6-4116-b6ac-e4189e3df41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafa109-776f-4fa9-8e5d-85c2046d7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\\n\")\n",
    "print(\"Books:\\n\", books.isnull().sum(), \"\\n\")\n",
    "print(\"Ratings:\\n\", ratings.isnull().sum(), \"\\n\")\n",
    "print(\"Book Tags:\\n\", book_tags.isnull().sum(), \"\\n\")\n",
    "print(\"Tags:\\n\", tags.isnull().sum(), \"\\n\")\n",
    "print(\"To Read:\\n\", to_read.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cde27f-3461-437e-b14b-88ee41b25bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ratings using Min-Max Scaling (0 to 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ratings['normalized_rating'] = scaler.fit_transform(ratings[['rating']])\n",
    "\n",
    "# Optional: show a few samples\n",
    "ratings[['user_id', 'book_id', 'rating', 'normalized_rating']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a21ca0-be7f-4084-9ecb-edadc36d1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(books.isnull().sum())\n",
    "print(ratings.isnull().sum())\n",
    "print(book_tags.isnull().sum())\n",
    "print(tags.isnull().sum())\n",
    "print(to_read.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddee2d-6562-4b55-ac1a-bb40ed3a30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "books['isbn'] = books['isbn'].fillna('0000000000')\n",
    "books['isbn13'] = books['isbn13'].fillna('0000000000000').astype(str)\n",
    "books['original_title'] = books['original_title'].fillna(books['title'])\n",
    "books['original_publication_year'] = books['original_publication_year'].fillna(books['original_publication_year'].median())\n",
    "books['language_code'] = books['language_code'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b898d-f000-4288-8e23-2e6913ea39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After Preprocessing:\")\n",
    "print(books.isnull().sum())\n",
    "print(ratings.isnull().sum())\n",
    "print(book_tags.isnull().sum())\n",
    "print(tags.isnull().sum())\n",
    "print(to_read.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4318c-d832-4511-a440-7f31b2f05ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.drop_duplicates(inplace=True)\n",
    "ratings.drop_duplicates(inplace=True)\n",
    "book_tags.drop_duplicates(inplace=True)\n",
    "tags.drop_duplicates(inplace=True)\n",
    "to_read.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4b69c-9ff6-418e-9ed3-6b4fb2160fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['book_id'].isin(books['book_id'])]\n",
    "to_read = to_read[to_read['book_id'].isin(books['book_id'])]\n",
    "book_tags = book_tags[book_tags['goodreads_book_id'].isin(books['book_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebb1ad-5feb-4d16-8c84-7cee2241a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ff26f-3d7e-4e04-a33c-1eb8f34a383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge book_tags with tags\n",
    "tagged_books = pd.merge(book_tags, tags, on=\"tag_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd831f-cdb1-418c-bc78-0ed96f2e51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d6db2-a249-424a-a909-6bdddbde3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge book_tags with tags\n",
    "book_tags_merged = pd.merge(book_tags, tags, on='tag_id', how='left')\n",
    "\n",
    "# Optional: Merge with books for enriched data\n",
    "book_tags_full = (\n",
    "    book_tags\n",
    "    .merge(tags, on='tag_id', how='left')\n",
    "    .merge(books, left_on='goodreads_book_id', right_on='best_book_id', how='inner')\n",
    ")\n",
    "\n",
    "book_tags_full[['title', 'tag_name', 'count']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48a244-a205-4c03-8baa-25503d3dfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_per_book = (\n",
    "    book_tags_full\n",
    "    .groupby('book_id')['tag_name']\n",
    "    .apply(lambda s: ' '.join(s.fillna('').astype(str)))\n",
    "    .reset_index())\n",
    "tags_per_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2acf3-0aea-4121-a4ce-dcb968dc6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='rating', data=ratings)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd25f80-89d5-4060-8aab-62ee56690f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get top 10 most rated book IDs from ratings\n",
    "top_book_ids = ratings['book_id'].value_counts().head(10)\n",
    "top_books_df = top_book_ids.reset_index()\n",
    "top_books_df.columns = ['book_id', 'num_ratings']  # renamed to avoid column clash\n",
    "\n",
    "# Step 2: Merge with books to get titles and authors\n",
    "top_books_merged = top_books_df.merge(books, on='book_id')\n",
    "\n",
    "# Step 3: Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y='title', x='num_ratings', data=top_books_merged, palette='plasma')\n",
    "plt.title('Top 10 Most Rated Books')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Book Title')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fedf4-82eb-4c66-aab7-00ebca41d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate mean ratings and rating counts\n",
    "mean_ratings = ratings.groupby('book_id')['rating'].mean()\n",
    "ratings_count = ratings['book_id'].value_counts()\n",
    "\n",
    "# Step 2: Filter only books with 100+ ratings\n",
    "popular_books = mean_ratings[ratings_count >= 100].sort_values(ascending=False).head(10)\n",
    "\n",
    "# Step 3: Get titles and authors (including book_id this time)\n",
    "top_rated = books[books['book_id'].isin(popular_books.index)][['book_id', 'title', 'authors']]\n",
    "\n",
    "# Step 4: Map the average ratings back\n",
    "top_rated['average_rating'] = top_rated['book_id'].map(mean_ratings)\n",
    "\n",
    "# Step 5: Sort and Plot\n",
    "top_rated = top_rated.sort_values('average_rating', ascending=False)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y='title', x='average_rating', data=top_rated, palette='viridis')\n",
    "\n",
    "plt.title('Top 10 Highest Rated Books (Min 100 Ratings)')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Book Title')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b58a0-8817-46ee-8b87-2e13c4b9e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_authors = books['authors'].value_counts().head(10)\n",
    "sns.barplot(y=popular_authors.index, x=popular_authors.values)\n",
    "plt.title('Most Frequent Authors in Dataset')\n",
    "plt.xlabel('Number of Books')\n",
    "plt.ylabel('Author')\n",
    "# Add value labels to bars\n",
    "for i, value in enumerate(popular_authors.values):\n",
    "    plt.text(value + 1, i, str(value), va='center')  # va = vertical alignment\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a0f7d-32f8-4d80-a7b8-f93b250f9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_counts = books['language_code'].value_counts().head(10)\n",
    "sns.barplot(x=lang_counts.index, y=lang_counts.values)\n",
    "plt.title('Top 10 Languages')\n",
    "plt.xlabel('Language Code')\n",
    "plt.ylabel('Number of Books')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecc1ac-714b-4eee-ab56-0060b24f1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# If you have a ratings DataFrame with numeric fields (e.g., rating, normalized_rating)\n",
    "numeric_cols = ratings.select_dtypes(include=['float64', 'int64'])\n",
    "corr = numeric_cols.corr()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd63317-da1f-4ef4-95a0-4b0db0ab4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popularity = number of ratings per book\n",
    "book_popularity = ratings.groupby('book_id').size().reset_index(name='rating_count')\n",
    "book_avg_rating = ratings.groupby('book_id')['rating'].mean().reset_index(name='avg_rating')\n",
    "\n",
    "popularity_df = book_popularity.merge(book_avg_rating, on='book_id')\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data=popularity_df, x='rating_count', y='avg_rating', alpha=0.6)\n",
    "plt.title(\"Book Popularity vs. Average Rating\")\n",
    "plt.xlabel(\"Number of Ratings\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f2d7d-d0ab-448c-ba8f-0589737aef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical summaries for ratings\n",
    "print(\"Overall Rating Stats:\")\n",
    "print(ratings['rating'].describe())\n",
    "\n",
    "# Ratings per book\n",
    "book_stats = ratings.groupby('book_id')['rating'].agg(['count', 'mean', 'median', 'std']).reset_index()\n",
    "book_stats.rename(columns={'count': 'num_ratings', 'mean': 'avg_rating', 'std': 'std_dev'}, inplace=True)\n",
    "\n",
    "print(\"\\nSample Book Rating Stats:\")\n",
    "print(book_stats.head())\n",
    "\n",
    "# Ratings per user\n",
    "user_stats = ratings.groupby('user_id')['rating'].agg(['count', 'mean', 'median', 'std']).reset_index()\n",
    "user_stats.rename(columns={'count': 'num_ratings', 'mean': 'avg_rating', 'std': 'std_dev'}, inplace=True)\n",
    "\n",
    "print(\"\\nSample User Rating Stats:\")\n",
    "print(user_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d1fee-00ff-4ccd-9e2d-83d75dee74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbd9f0-aff3-4ef6-a79f-3233bae78608",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd1a09-d814-447a-8a38-2e0c40c9d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d145ae1-2a58-4982-87d9-9c702b090bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940629f4-4f19-4711-913f-c1983f920cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dff78-919d-4da8-9e72-c6fd9aa4211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_full.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cce7e-93ee-4a4b-9b35-b003645cfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and processed dataframes to new CSV files\n",
    "books.to_csv('cleaned_books.csv', index=False)\n",
    "ratings.to_csv('cleaned_ratings.csv', index=False)\n",
    "book_tags_full.to_csv('cleaned_book_tags.csv', index=False)\n",
    "to_read.to_csv('cleaned_to_read.csv', index=False)\n",
    "tagged_books.to_csv('tagged_books.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f4d38-17c2-4a08-bbbe-63a22747b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned ratings\n",
    "ratings = pd.read_csv(\"cleaned_ratings.csv\")\n",
    "\n",
    "# Reader (rating scale from 1â€“5)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Use full dataset (but you can sample for user-based CF to avoid memory issues)\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528350e-3eea-4296-8f29-fc4a17641228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned ratings\n",
    "ratings = pd.read_csv(\"cleaned_ratings.csv\")\n",
    "\n",
    "# Reader with rating scale\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# ---------------------------\n",
    "# ITEM-BASED COLLABORATIVE FILTERING\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Item-Based CF (Full Dataset)\")\n",
    "data_item = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset_item, testset_item = train_test_split(data_item, test_size=0.2)\n",
    "\n",
    "sim_options_item = {'name': 'cosine', 'user_based': False}   # Item-based\n",
    "item_cf = KNNBasic(sim_options=sim_options_item, verbose=True)\n",
    "item_cf.fit(trainset_item)\n",
    "predictions_item = item_cf.test(testset_item)\n",
    "\n",
    "print(\"Item-Based CF Results:\")\n",
    "accuracy.rmse(predictions_item)\n",
    "accuracy.mae(predictions_item)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# USER-BASED COLLABORATIVE FILTERING (on sample to avoid memory error)\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ User-Based CF (Sampled Dataset)\")\n",
    "\n",
    "# Take 10% of ratings for user-based (to reduce matrix size)\n",
    "sampled_ratings = ratings.sample(frac=0.1, random_state=42)\n",
    "data_user = Dataset.load_from_df(sampled_ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset_user, testset_user = train_test_split(data_user, test_size=0.2)\n",
    "\n",
    "sim_options_user = {'name': 'cosine', 'user_based': True}    # User-based\n",
    "user_cf = KNNBasic(sim_options=sim_options_user, verbose=True)\n",
    "user_cf.fit(trainset_user)\n",
    "predictions_user = user_cf.test(testset_user)\n",
    "\n",
    "print(\"User-Based CF Results (on sample):\")\n",
    "accuracy.rmse(predictions_user)\n",
    "accuracy.mae(predictions_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1b7b6-c2b3-4ce8-a7d9-9b5278b69cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, NMF\n",
    "\n",
    "# SVD\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "predictions_svd = svd.test(testset)\n",
    "print(\"SVD\")\n",
    "accuracy.rmse(predictions_svd)\n",
    "accuracy.mae(predictions_svd)\n",
    "\n",
    "# NMF\n",
    "nmf = NMF()\n",
    "nmf.fit(trainset)\n",
    "predictions_nmf = nmf.test(testset)\n",
    "print(\"NMF\")\n",
    "accuracy.rmse(predictions_nmf)\n",
    "accuracy.mae(predictions_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980530e-90b0-486f-b6b0-a54f5c1df61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to get Top-N predictions\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "# Precision, Recall, F1 at K\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort by estimated rating\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_k = user_ratings[:k]\n",
    "\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)     # all relevant\n",
    "        n_rel_and_rec = sum((true_r >= threshold) for (_, true_r) in top_k)    # relevant in top-k\n",
    "\n",
    "        prec = n_rel_and_rec / k if k else 0\n",
    "        rec = n_rel_and_rec / n_rel if n_rel else 0\n",
    "        f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0\n",
    "\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return (sum(precisions) / len(precisions),\n",
    "            sum(recalls) / len(recalls),\n",
    "            sum(f1s) / len(f1s))\n",
    "\n",
    "# Evaluate all models\n",
    "for name, preds in [\n",
    "    (\"User CF\", predictions_user),\n",
    "    (\"Item CF\", predictions_item),\n",
    "    (\"SVD\", predictions_svd),\n",
    "    (\"NMF\", predictions_nmf)\n",
    "]:\n",
    "    p, r, f1 = precision_recall_at_k(preds, k=10, threshold=3.5)\n",
    "    print(f\"{name} -> Precision@10: {p:.4f}, Recall@10: {r:.4f}, F1@10: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f837bb5-c91e-4644-98d5-a5b00e310bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 3: Content-Based Recommendation System\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "\n",
    "# Load your cleaned data\n",
    "books = pd.read_csv(\"cleaned_books.csv\")\n",
    "ratings = pd.read_csv(\"cleaned_ratings.csv\")\n",
    "book_tags_full = pd.read_csv(\"cleaned_book_tags.csv\")\n",
    "tagged_books = pd.read_csv(\"tagged_books.csv\")\n",
    "\n",
    "# ========================\n",
    "# 1. CONTENT-BASED RECOMMENDATION SYSTEM\n",
    "# ========================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Create content features for books\n",
    "def create_content_features(books_df, book_tags_df):\n",
    "    \"\"\"\n",
    "    Create content-based features by combining:\n",
    "    - Book titles\n",
    "    - Authors\n",
    "    - Tags/genres\n",
    "    \"\"\"\n",
    "    content_df = books_df.copy()\n",
    "    \n",
    "    # Preprocess text fields\n",
    "    content_df['clean_title'] = content_df['title'].apply(preprocess_text)\n",
    "    content_df['clean_authors'] = content_df['authors'].apply(preprocess_text)\n",
    "    \n",
    "    # Get tags for each book\n",
    "    book_tags_agg = (book_tags_df.groupby('book_id')['tag_name']\n",
    "                     .apply(lambda x: ' '.join(x.fillna('').astype(str)))\n",
    "                     .reset_index())\n",
    "    \n",
    "    # Merge tags with books\n",
    "    content_df = content_df.merge(book_tags_agg, on='book_id', how='left')\n",
    "    content_df['tag_name'] = content_df['tag_name'].fillna('')\n",
    "    content_df['clean_tags'] = content_df['tag_name'].apply(preprocess_text)\n",
    "    \n",
    "    # Combine all text features\n",
    "    content_df['combined_features'] = (\n",
    "        content_df['clean_title'] + ' ' + \n",
    "        content_df['clean_authors'] + ' ' + \n",
    "        content_df['clean_tags']\n",
    "    )\n",
    "    \n",
    "    return content_df\n",
    "\n",
    "# Create content features\n",
    "content_books = create_content_features(books, book_tags_full)\n",
    "print(\"Content features created!\")\n",
    "print(f\"Sample combined features:\\n{content_books[['title', 'combined_features']].head()}\")\n",
    "\n",
    "# ========================\n",
    "# 2. TF-IDF VECTORIZATION\n",
    "# ========================\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,      # Limit features to avoid memory issues\n",
    "    stop_words='english',   # Remove common English words\n",
    "    ngram_range=(1, 2),     # Use unigrams and bigrams\n",
    "    min_df=2,               # Ignore terms appearing in less than 2 documents\n",
    "    max_df=0.8              # Ignore terms appearing in more than 80% of documents\n",
    ")\n",
    "\n",
    "# Fit TF-IDF on combined features\n",
    "tfidf_matrix = tfidf.fit_transform(content_books['combined_features'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# ========================\n",
    "# 3. CONTENT-BASED RECOMMENDATION FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def get_content_recommendations(book_id, books_df, tfidf_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Get content-based recommendations for a given book\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the index of the book\n",
    "        book_idx = books_df[books_df['book_id'] == book_id].index[0]\n",
    "        \n",
    "        # Calculate cosine similarity with all other books\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[book_idx:book_idx+1], tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get similarity scores and sort\n",
    "        sim_scores = list(enumerate(cosine_sim))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N similar books (excluding the input book itself)\n",
    "        sim_scores = sim_scores[1:top_n+1]\n",
    "        \n",
    "        # Get book indices and similarities\n",
    "        book_indices = [i[0] for i in sim_scores]\n",
    "        similarities = [i[1] for i in sim_scores]\n",
    "        \n",
    "        # Return recommended books with similarity scores\n",
    "        recommendations = books_df.iloc[book_indices][['book_id', 'title', 'authors']].copy()\n",
    "        recommendations['similarity_score'] = similarities\n",
    "        \n",
    "        return recommendations\n",
    "        \n",
    "    except IndexError:\n",
    "        print(f\"Book ID {book_id} not found in dataset\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_user_content_recommendations(user_id, ratings_df, books_df, tfidf_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Get content-based recommendations for a user based on their reading history\n",
    "    \"\"\"\n",
    "    # Get books rated highly by the user (rating >= 4)\n",
    "    user_books = ratings_df[(ratings_df['user_id'] == user_id) & \n",
    "                           (ratings_df['rating'] >= 4)]['book_id'].tolist()\n",
    "    \n",
    "    if not user_books:\n",
    "        print(f\"No high-rated books found for user {user_id}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get content recommendations for each book the user liked\n",
    "    all_recommendations = []\n",
    "    \n",
    "    for book_id in user_books[:5]:  # Limit to top 5 liked books to avoid overwhelming\n",
    "        recs = get_content_recommendations(book_id, books_df, tfidf_matrix, top_n=20)\n",
    "        if not recs.empty:\n",
    "            all_recommendations.append(recs)\n",
    "    \n",
    "    if not all_recommendations:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all recommendations\n",
    "    combined_recs = pd.concat(all_recommendations, ignore_index=True)\n",
    "    \n",
    "    # Remove books the user has already rated\n",
    "    user_rated_books = ratings_df[ratings_df['user_id'] == user_id]['book_id'].tolist()\n",
    "    combined_recs = combined_recs[~combined_recs['book_id'].isin(user_rated_books)]\n",
    "    \n",
    "    # Group by book and average similarity scores\n",
    "    final_recs = (combined_recs.groupby(['book_id', 'title', 'authors'])['similarity_score']\n",
    "                  .mean().reset_index())\n",
    "    \n",
    "    # Sort by similarity score and return top N\n",
    "    final_recs = final_recs.sort_values('similarity_score', ascending=False).head(top_n)\n",
    "    \n",
    "    return final_recs\n",
    "\n",
    "# ========================\n",
    "# 4. TEST CONTENT-BASED RECOMMENDATIONS\n",
    "# ========================\n",
    "\n",
    "# Test book-to-book recommendations\n",
    "sample_book_id = content_books['book_id'].iloc[0]\n",
    "print(f\"\\nðŸ“š Content-based recommendations for book ID {sample_book_id}:\")\n",
    "print(f\"Book: {content_books[content_books['book_id'] == sample_book_id]['title'].iloc[0]}\")\n",
    "\n",
    "book_recs = get_content_recommendations(sample_book_id, content_books, tfidf_matrix, top_n=5)\n",
    "print(book_recs)\n",
    "\n",
    "# Test user-based content recommendations\n",
    "sample_user = ratings['user_id'].iloc[0]\n",
    "print(f\"\\nðŸ‘¤ Content-based recommendations for user {sample_user}:\")\n",
    "\n",
    "user_recs = get_user_content_recommendations(sample_user, ratings, content_books, tfidf_matrix, top_n=5)\n",
    "print(user_recs)\n",
    "\n",
    "# ========================\n",
    "# 5. EVALUATION METRICS FOR CONTENT-BASED SYSTEM\n",
    "# ========================\n",
    "\n",
    "def evaluate_content_based_system(ratings_df, books_df, tfidf_matrix, sample_users=100):\n",
    "    \"\"\"\n",
    "    Evaluate content-based system using precision and coverage metrics\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Sample random users for evaluation\n",
    "    unique_users = ratings_df['user_id'].unique()\n",
    "    sample_user_ids = np.random.choice(unique_users, min(sample_users, len(unique_users)), replace=False)\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_coverage = 0\n",
    "    successful_recs = 0\n",
    "    \n",
    "    for user_id in sample_user_ids:\n",
    "        # Get recommendations\n",
    "        recs = get_user_content_recommendations(user_id, ratings_df, books_df, tfidf_matrix, top_n=10)\n",
    "        \n",
    "        if recs.empty:\n",
    "            continue\n",
    "            \n",
    "        # Get user's actual high ratings (test set)\n",
    "        user_high_rated = ratings_df[(ratings_df['user_id'] == user_id) & \n",
    "                                   (ratings_df['rating'] >= 4)]['book_id'].tolist()\n",
    "        \n",
    "        if len(user_high_rated) < 2:  # Need at least 2 books for meaningful evaluation\n",
    "            continue\n",
    "            \n",
    "        # Calculate precision (how many recommended books are actually liked)\n",
    "        recommended_books = recs['book_id'].tolist()\n",
    "        relevant_recs = len(set(recommended_books) & set(user_high_rated))\n",
    "        precision = relevant_recs / len(recommended_books) if recommended_books else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        successful_recs += 1\n",
    "    \n",
    "    avg_precision = total_precision / successful_recs if successful_recs > 0 else 0\n",
    "    \n",
    "    print(f\"Content-Based System Evaluation:\")\n",
    "    print(f\"Average Precision@10: {avg_precision:.4f}\")\n",
    "    print(f\"Successfully evaluated users: {successful_recs}/{len(sample_user_ids)}\")\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "# Evaluate the content-based system\n",
    "content_precision = evaluate_content_based_system(ratings, content_books, tfidf_matrix, sample_users=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
